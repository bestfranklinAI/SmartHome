{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('database.json') as file:\n",
    "    data = pd.read_json(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.style.set_properties(subset=['weather_forecast'], **{'width': '500px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"future_temp\"] = data[\"weather_forecast\"].apply(lambda x: x[\"temp\"])\n",
    "data[\"future_humidity\"] = data[\"weather_forecast\"].apply(lambda x: x[\"humidity\"])\n",
    "data[\"future_uv\"] = data[\"weather_forecast\"].apply(lambda x: x[\"uv_index\"])\n",
    "data[\"future_rainfall\"] = data[\"weather_forecast\"].apply(lambda x: x[\"rainfall\"])\n",
    "data[\"future_wind_speed\"] = data[\"weather_forecast\"].apply(lambda x: x[\"wind_speed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.style.set_properties(subset=['weather_forecast'], **{'width': '500px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [1 ,2 ,3 ,7 ,8 ,9 , 11, 12, 13, 14, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = data.iloc[:,selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = np.mean(selected_data, axis=0)\n",
    "data_sd = np.std(selected_data, axis=0)\n",
    "adjusted_data = (selected_data - data_mean) / data_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center\"> Data Splitting<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data_numpy(X, y, numpy_seed):\n",
    "    # fix the random seed\n",
    "    np.random.seed(numpy_seed)\n",
    "\n",
    "    # TODO Task 1.1\n",
    "    # shuffle the given data pair (X, y)\n",
    "    # please use numpy functions so that the results are controled by np.random.seed(numpy_seed)\n",
    "    shuffled_array = np.random.permutation(X.shape[0])\n",
    "    X_shuffle = X[shuffled_array]\n",
    "    y_shuffle = y[shuffled_array]\n",
    "    \n",
    "\n",
    "    return X_shuffle, y_shuffle\n",
    "\n",
    "def train_val_split(X_trainval, y_trainval, train_size, numpy_seed):\n",
    "    # TODO TASK 1.2 \n",
    "    # apply shuffle on the data with given random seed, then split the data into training and validation sets\n",
    "    \n",
    "    X_shuffle , y_shuffle = shuffle_data_numpy(X_trainval, y_trainval, numpy_seed)\n",
    "    X_train = X_shuffle[:train_size]\n",
    "    y_train = y_shuffle[:train_size]\n",
    "    X_val = X_shuffle[train_size:]\n",
    "    y_val = y_shuffle[train_size:]\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = np.array([[26,10],[22, 6], [24, 7], [26, 8], [22, 6], [27, 8], [19, 0], [18, 2], [26, 5], [32, 8], [31, 9], [21, 2], [22, 8], [15, 2], [21, 2], [34, 10], [27, 7], [21, 2], [21, 4], [15, 1], [24, 3], [31, 3], [34, 4], [11, 0], [24, 5], [30, 7], [33,8], [13, 1], [23, 7], [29, 6], [30, 8], [25, 3], [25, 3], [13, 0], [23, 1], [32, 2], [34, 1], [11, 0], [33, 10], [26, 4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_mean = np.mean(truth, axis=0)\n",
    "truth_sd = np.std(truth, axis=0)\n",
    "adjusted_truth = (truth - truth_mean )/ truth_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_val_split(np.array(adjusted_data), np.array(adjusted_truth), int(adjusted_data.shape[0]*0.7), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(X_val)\n",
    "print(y_train)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No additional import allowed\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.layers import Dense\n",
    "\n",
    "def MyModel(input_dim, dropout_ratio):\n",
    "    # Create a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    model.build((None,input_dim))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Dense(units = 64, activation = 'relu', kernel_initializer = \"uniform\"))\n",
    "    model.add(Dropout(rate = dropout_ratio))\n",
    "    \n",
    "    model.add(Dense(units = 32, activation = 'relu', kernel_initializer = \"uniform\"))\n",
    "    model.add(Dropout(rate = dropout_ratio))\n",
    "    \n",
    "    model.add(Dense(units = 16, activation = 'relu', kernel_initializer = \"uniform\"))\n",
    "    model.add(Dropout(rate = dropout_ratio))\n",
    "    \n",
    "    model.add(Dense(units = 4, activation = 'relu', kernel_initializer = \"uniform\"))\n",
    "    model.add(Dropout(rate = dropout_ratio))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer = \"uniform\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep them as the default setting for the model you submitted to ZINC!\n",
    "input_dim = len(selected)\n",
    "dropout_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def MyModel_Training(model, X_train, y_train, X_val, y_val, batchsize, train_epoch):\n",
    "\n",
    "    # TODO Task 2.2\n",
    "    # Compile and train the given model\n",
    "    # Hint: history can be returned by model.fit() function, please see https://keras.io/api/models/model_training_apis/\n",
    "    adam_optimizer = Adam(learning_rate = 1e-3)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer= adam_optimizer, \n",
    "        loss = 'mse',\n",
    "        metrics =['mae'])\n",
    "    \n",
    "    history = model.fit(x = X_train, y = y_train, batch_size = batchsize, epochs = train_epoch, validation_data = (X_val, y_val))\n",
    "    \n",
    "    \n",
    "    return history, model\n",
    "\n",
    "model = MyModel(input_dim, dropout_ratio)\n",
    "\n",
    "batchsize = 4\n",
    "train_epoch = 50\n",
    "\n",
    "history, model = MyModel_Training(model, X_train, y_train[:,0], X_val, y_val[:,0], batchsize, train_epoch)\n",
    "test_loss, test_mae = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(f'Test Mean Average Error (MAE): {test_mae}')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [38.5, 80, 10000, 9, 5, 2, 40.5, 90, 6, 0, 0]\n",
    "adjusted_test = (test - data_mean) / data_sd\n",
    "prediction = model.predict(np.array(adjusted_test).reshape(1, -1))\n",
    "prediction*truth_sd[0] + truth_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./smartHome.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['mae'], label='Training mae')\n",
    "plt.plot(history.history['val_mae'], label='Validation mae')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_val).flatten()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_val[:,0], test_predictions)\n",
    "plt.xlabel('Ground True Values for optimal Air conditioning temperature')\n",
    "plt.ylabel('Predictions for Air conditioning temperature)')\n",
    "# plt.axis('equal')\n",
    "# plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-100, 3000], [-100, 3000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
